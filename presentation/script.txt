kNN: From Slow to Fast using Math

Problem:
Given a set of objects S find the k nearest neighbours in S for a every posbile object
Objects are represented by feature vectors
The similarity function is cosine similarity

Solution
Compute a feature vector for every objects in S
When the k nearest neighbours for an object represented by feature vector X is requested
Compute the cosine similarity of X with every feature vector in S
Sort by similariy ascending, return the last k elements.


Analysis
Sorting Complexity: O(nlogn)
Distance evaluation Complexity: O(n)
Total Time Complexity: AO(nlogn) + BO(n), B>>A ==> O(n) 
A heap does not help: Distance evaluation complexity is O(nlogk)
QuickSelect does not help: Distance evaluation complexity is O(n) on avg but O(n**2) worst case.


How about a Space Partitioning Tree?
It does not work: cosine similarity is not a metric because it does not hold the triangle inequality.

Math

Cosine distances is invariant under normalization.
If A and B are unit vectors then the euclidean distance between vector A and B is exatly 2-2cos(A,B)

Consider three vectors, X, A and, B such that:
1-cos(X, A) > 1-cos(X, B)

1-cos(X/|X|, A/|A|) < 1-cos(X/|X|, B/|B|)

2-2cos(X/|X|, A/|A|) < 2-2cos(X/|X|, B/|B|)

||X/|X|-A/|A||| < ||X/|x|-B/|B|||

eculideansim(X/|X|, B/|B|) > euclideansim(X/|X|,A/|A|)

Trick
Normalize all the feature vectors of S
Build a SPT using euclidean distance
When the k nearest neighbours for an object represented by feature vector X is requested
Normalize X, and look up the k nearest neighbours using the SPT.

Experiments


Missing Values in Classification: From bad to good using an old truth

Problem



Clustering: from darkness to light 
Problem
Given a set of high dimensional objects and cluster assignements, interpret the clusters by visualizing them.

Solutions
2D Representation
Dendogram
CLustergrams

Analysis

Trick
Visualize the clusters, 


Head tail breaks

Total trick

Situation
We are bulding a Classifier
The classifier outputs the Pr(Class|feature 1, feature 2, ..., feature k)
At prediction time, some features might be missing for some examples

Solutions

Fill in with the most common value
Fill in with the Expected value
...
Build a machine learning algorithm to predict the missing value

Analysis
All these solutions try to predict the missing value.
Some of them are not applicable for categorical data.
Most of them are too simplisitic or too expensive to build.


Math
A more natural set up of the problem:
Given a classifier that outputs Pr(Class | fa, fb, fc)
Build a classifier that outputs Pr(Class | fa, fc)

Conditional Law of Total Probability
If C and B are independent then:
Pr(A|C) =  sum over B of Pr(A|B,C)Pr(B)

Trick
Assume fa, fb, and fc are all independent from each other.
Estimate Pr(Class | fa=a1, fc=c7) as
sum over i Pr(Class | fa=a1, fb=bi, fc=c7)Pr(fb=bi)

Experiments



Head Tail Breaks
Situation
Problem
We have data. Every data point is represented by a feature vector.
There is one specific feature that we want to cluster in to groups.
AKA: Discretization, Bining, Quantization, 1d-clustering

Solutions
Quantile Binning
Jenks Natural Breaks (1d k-means)
Supervised Methods

Analysis
Many of these methods take the amount of clusters as input
Supervised methods are only suitable for specific tasks
Some methods are not robust to different distribution types

Math

Power Law

Pareto Distribution

Rank Size Distribution

Log-normal Distribution

Given a variable X, if its values x follow a heavy tailed distribution, then the mean of the values can divide all the values into two parts: a high percentage in the tail, and a low percentage in the head.


Trick

def head_tail_breaks(data, breaks=[]):
	n = len(data)
	m = mean(data)
	head = [x for x in data if x>m]
	tail = [x for x in data if x<=m]
	breaks.append(m)
	if len(head)<n*0.4 and len(head)>0:
		return head_tail_breaks(head, breaks)
	else:
		return breaks	

Experiment

Predict the price of houses, based on several features
Apply head tail breaks to every feature
Discretize the features using the computed breaks
Use the discretized features in the regression

Results






 












